accuracy_mod1
accuracy_mod2
accuracy_mod3
install.packages("gbm")
install.packages("AppliedPredictiveModeling")
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
install.packages("e1071")
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
## QUESTION TWO
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
rm(list=ls())
#load Alzheimer's data
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
testinga<-testing
testinga$diagnosis<-as.numeric(testing$diagnosis)-1
# Build three different models
set.seed(62433)
mod1 <- train(diagnosis ~.,method="gbm",data=training)
mod2 <- train(diagnosis ~.,method="rf",data=training)
mod3 <- train(diagnosis ~.,method="lda",data=training)
#Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set?
# Is it better or worse than each of the individual predictions?
# Predict on the testing set
pred1 <- predict(mod1,testing)
pred2 <- predict(mod2,testing)
pred3 <- predict(mod3,testing)
# Accuracies of each model
ctmod1_test<-table(testing$diagnosis, pred1)
accuracy_mod1<-sum(diag(ctmod1_test))/sum(ctmod1_test)
accuracy_mod1
ctmod2_test<-table(testing$diagnosis, pred2)
accuracy_mod2<-sum(diag(ctmod2_test))/sum(ctmod2_test)
accuracy_mod2
ctmod3_test<-table(testing$diagnosis, pred3)
accuracy_mod3<-sum(diag(ctmod3_test))/sum(ctmod3_test)
accuracy_mod3
# stack the models
dfStack<-data.frame(pred1,pred2,pred3,diagnosis=testing$diagnosis)
#fit a model using rf that combines the predictors
set.seed(62433)
combModFit <- train(diagnosis ~.,method="rf",
data=dfStack)
combPred <- predict(combModFit,dfStack)
# combined accuracy
ct_comb<-table(testing$diagnosis, combPred)
accuracy_comb<-sum(diag(ct_comb))/sum(ct_comb)
accuracy_comb
accuracy_mod1
accuracy_mod2
accuracy_mod3
setwd("H:/Rspace/Debs")
## Data from
## https://docs.google.com/spreadsheets/d/1z4ffKuH6AW0iWIhTMA1RrAjaBziS2LEKzUeUD7FMVx0/edit#gid=2002317430
## read in data
pay<-read.csv("academicPay.csv",stringsAsFactors=FALSE)
str(pay)
pay<-subset(pay,mfAcademicPayRatio>0.75)
Falmouth.mf.ratio<-subset(pay,University=="Falmouth University")$mfAcademicPayRatio
## Plot distribution of pay ratios
library(ggplot2)
library(grid)
png("mfPayRatio.png",width = 600, height = 480)
g<-ggplot(pay,aes(x=mfAcademicPayRatio,fill=..x..),alpha=0.2)+
#geom_histogram(binwidth=0.02)+
geom_histogram(binwidth=0.02)+
coord_flip()+
#scale_x_reverse()+
scale_fill_gradient("Ratio", low = "green", high = "red")+
#geom_vline(aes(xintercept=Falmouth.mf.ratio),   # Ignore NA values for mean
#color="red", linetype="dashed", size=1)+
labs(x = "Ratio of male academic pay to female academic pay",y = "Number of Universities")+
geom_segment(aes(x =1.0 , y = 7, xend = 1.0, yend = 25),color="black",linewidth=20,linetype="dashed")+
annotate("text", x = Falmouth.mf.ratio, y = 4., label = "Falmouth",color="blue",size=8)+
geom_segment(aes(x = Falmouth.mf.ratio, y = 1.5, xend = Falmouth.mf.ratio, yend = 0),color="blue",linewidth=20, arrow = arrow(type="closed",length = unit(0.4, "cm"),))+
theme(axis.text.x = element_text(size=16),
axis.text.y=element_text(size=16))+
theme(axis.title.x = element_text(size=16,vjust=-.5),
axis.title.y=element_text(size=16,vjust=1.2))+
theme(legend.text=element_text(size=14),
legend.title=element_text(size=14))
g
dev.off()
falmouth<-subset(pay,pay$University=="Falmouth University")
library(dplyr)
arrange(pay,mfAcademicPayRatio)
pay$deviance<-abs(1-pay$mfAcademicPayRatio)
arrange(pay,deviance)
ggplot(pay,aes(x=deviance,fill=..x..),alpha=0.2)+
#geom_histogram(binwidth=0.02)+
geom_histogram(binwidth=0.02)+
scale_fill_gradient("Deviation", low = "green", high = "red")+
geom_vline(aes(xintercept=0.00),   # Ignore NA values for mean
color="red", linetype="dashed", size=1)+
labs(x = "Deviation from gender parity of pay of all academics",y = "Count")
deviance<-data.frame(arrange(pay,deviance))
deviance
write.csv("deviance.csv")
?write.csv
write.csv(data=deviance,"deviance.csv")
write.table(data=deviance,"deviance.csv",sep=",",row.names=TRUE)
write.table(deviance,"deviance.csv",sep=",",row.names=TRUE)
